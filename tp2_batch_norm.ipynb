{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp -r drive/MyDrive/IFT780/TP2/prog/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TP2: Batch normalisation [1]\n",
    "\n",
    "Il n'y a rien à coder à proprement parler dans ce notebook.  Par contre, assurez-vous de bien comprendre comment intégrer une opération de batch-norm à un modèle car vous en aurez besoin dans le notebook **cnn**.\n",
    "\n",
    "[1] Sergey Ioffe and Christian Szegedy, \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "Internal Covariate Shift\", ICML 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "from utils.gradients import evaluate_numerical_gradient\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cicles dev\n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test, label_names = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "def preprocess_CIFAR10_data(X):\n",
    "\n",
    "    # Normalisation\n",
    "    X_mean = np.mean(X, axis = 0)\n",
    "    X_ = X - X_mean\n",
    "\n",
    "    # Ajout du biais\n",
    "    X_ = np.hstack([X_, np.ones((X.shape[0], 1))])\n",
    "    \n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3072)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=\"5\">BatchNorm</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=\"3\">Forward</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means [-15.65636558 -51.62665966  11.56595992]\n",
      "stds [34.8506054  33.03919962 39.74692281]\n",
      "means [ 1.15463195e-16 -7.23032745e-17  3.30291350e-17]\n",
      "stds [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from model.Model import Model\n",
    "from layers.BatchNorm import BatchNorm\n",
    "from utils.model_loss import cross_entropy_loss\n",
    "\n",
    "def create_data():\n",
    "    np.random.seed(1)\n",
    "    N, D1, D2, D3 = 200, 50, 60, 3\n",
    "    X = np.random.randn(N, D1)\n",
    "    W1 = np.random.randn(D1, D2)\n",
    "    W2 = np.random.randn(D2, D3)\n",
    "    a = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "    return a\n",
    "\n",
    "data = create_data()\n",
    "print('means', data.mean(axis=0))\n",
    "print('stds', data.std(axis=0))\n",
    "\n",
    "def create_toy_model(weight_scale):\n",
    "    np.random.seed(0)\n",
    "    model = Model()\n",
    "    model.add(BatchNorm(3, weight_scale=weight_scale))\n",
    "    model.add_loss(cross_entropy_loss)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_toy_model(None)\n",
    "data_norm = model.forward(data)\n",
    "print('means', data_norm.mean(axis=0))\n",
    "print('stds', data_norm.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=\"3\">Rétro-propagation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.67973668 19.63952676  7.14644531]\n",
      " [14.35279808 11.49651642 13.51896592]\n",
      " [ 3.37018786 19.92547685 12.67148297]\n",
      " [ 6.46572267 19.89112775 12.53748971]\n",
      " [ 8.17976087  8.12405745 18.91923583]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'W'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n\u001b[1;32m     18\u001b[0m scores \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m---> 19\u001b[0m loss, dScores, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m dX \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbackward(dScores)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(dX)\n",
      "File \u001b[0;32m~/tmp/progTP2/model/Model.py:81\u001b[0m, in \u001b[0;36mModel.calculate_loss\u001b[0;34m(self, model_output, targets, reg)\u001b[0m\n\u001b[1;32m     77\u001b[0m     layer\u001b[38;5;241m.\u001b[39mreg \u001b[38;5;241m=\u001b[39m reg\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Retourne un tuple contenant la loss et les gradients de la loss par\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# rapport au output de la derniere couche.\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tmp/progTP2/utils/model_loss.py:41\u001b[0m, in \u001b[0;36mcross_entropy_loss\u001b[0;34m(scores, t, reg, model_params)\u001b[0m\n\u001b[1;32m     39\u001b[0m     key,i\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m) :\n\u001b[0;32m---> 41\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m reg \u001b[38;5;241m*\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(i[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     42\u001b[0m dScores \u001b[38;5;241m=\u001b[39m softmax_output\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     43\u001b[0m dScores[np\u001b[38;5;241m.\u001b[39marange(N), t] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W'"
     ]
    }
   ],
   "source": [
    "model = create_toy_model(1e-4)\n",
    "\n",
    "def create_data():\n",
    "    np.random.seed(30)\n",
    "    X = 5 * np.random.randn(5, 3) + 12\n",
    "    y = np.array([1,0,2,1,0])\n",
    "    return (X, y)\n",
    "\n",
    "correct_out = [[ 1.85344593e-06, -5.74238814e-07,  1.17228483e-06],\n",
    "               [ 1.11060921e-06,  2.05814890e-07,  1.03496010e-06],\n",
    "               [-4.81622969e-07,  1.04901527e-06, -4.18244822e-06],\n",
    "               [ 2.64791282e-06, -5.49104141e-07,  1.05625858e-06],\n",
    "               [-5.13034500e-06, -1.31487208e-07,  9.18944701e-07]]\n",
    "\n",
    "X, y = create_data()\n",
    "\n",
    "print(X)\n",
    "scores = model.forward(X)\n",
    "loss, dScores, _ = model.calculate_loss(scores, y, 0.1)\n",
    "dX = model.backward(dScores)\n",
    "print(dX)\n",
    "\n",
    "# Pour chaque paramètre, l'erreur relative devrait être inférieure à environ 1e-8.\n",
    "def rel_error(x, y):\n",
    "    rel = np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y)))\n",
    "    return np.max(rel)\n",
    "print('difference: ', rel_error(dX, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=\"3\">Forward en mode test</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = create_toy_model(None)\n",
    "\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    X = np.random.randn(N, D1)\n",
    "    Z = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "    model.forward(Z)\n",
    "    \n",
    "X = np.random.randn(N, D1)\n",
    "Z = np.maximum(0, X.dot(W1)).dot(W2)\n",
    "\n",
    "A_norm = model.forward(Z, mode=\"test\")\n",
    "\n",
    "print('means', A_norm.mean(axis=0))\n",
    "print('stds', A_norm.std(axis=0))\n",
    "\n",
    "correct_means = [-0.0997174,  -0.02081184, -0.05757337]\n",
    "corret_stds = [1.00571489, 1.06003059, 1.06759498]\n",
    "\n",
    "print('mean difference: ', rel_error(A_norm.mean(axis=0), correct_means))\n",
    "print('std difference: ', rel_error(A_norm.std(axis=0), corret_stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=\"5\">Réseau multi-couches sans batch norm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model.Solver import epoch_solver, Adam\n",
    "from layers.Dense import Dense\n",
    "\n",
    "num_train = 1000\n",
    "\n",
    "train_accuracy_histories = []\n",
    "val_accuracy_histories = []\n",
    "loss_histories = []\n",
    "\n",
    "X_train_small = X_train[:num_train]\n",
    "y_train_small = y_train[:num_train]\n",
    "\n",
    "def create_model(weight_scale):\n",
    "    model = Model()\n",
    "    classes = 10\n",
    "    hidden_size = 100\n",
    "    \n",
    "    dense1 = Dense(dim_output=hidden_size, weight_scale=weight_scale, activation='relu')\n",
    "    dense2 = Dense(dim_input=hidden_size, dim_output=hidden_size, weight_scale=weight_scale, activation='relu')\n",
    "    dense3 = Dense(dim_input=hidden_size, dim_output=hidden_size, weight_scale=weight_scale, activation='relu')\n",
    "    dense4 = Dense(dim_input=hidden_size, dim_output=hidden_size, weight_scale=weight_scale, activation='relu')\n",
    "    dense5 = Dense(dim_input=hidden_size, dim_output=classes, weight_scale=weight_scale)\n",
    "    \n",
    "    model.add(dense1, 'dense1')\n",
    "    model.add(dense2, 'dense2')\n",
    "    model.add(dense3, 'dense3')\n",
    "    model.add(dense4, 'dense4')\n",
    "    model.add(dense5, 'dense5')\n",
    "    model.add_loss(cross_entropy_loss)\n",
    "    optimizer = Adam(1e-3, model)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = create_model(1e-2)\n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver(X_train_small, \n",
    "                                                                          y_train_small,\n",
    "                                                                          X_val,\n",
    "                                                                          y_val,\n",
    "                                                                          0.0,\n",
    "                                                                          optimizer,\n",
    "                                                                          epochs=10,\n",
    "                                                                          batch_size=50)\n",
    "\n",
    "loss_histories.append(loss_history)\n",
    "train_accuracy_histories.append(train_accuracy_history)\n",
    "val_accuracy_histories.append(val_accuracy_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=\"5\">Réseau multi-couches AVEC batch norm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from layers.BatchNorm import BatchNorm\n",
    "def create_normalized_model(weight_scale):\n",
    "    model = Model()\n",
    "    classes = 10\n",
    "    hidden_size = 100\n",
    "    \n",
    "    dense1 = Dense(dim_output=hidden_size, weight_scale=weight_scale)\n",
    "    dense2 = Dense(dim_input=hidden_size, dim_output=hidden_size, weight_scale=weight_scale)\n",
    "    dense3 = Dense(dim_input=hidden_size, dim_output=hidden_size, weight_scale=weight_scale)\n",
    "    dense4 = Dense(dim_input=hidden_size, dim_output=hidden_size, weight_scale=weight_scale)\n",
    "    dense5 = Dense(dim_input=hidden_size, dim_output=classes, weight_scale=weight_scale)\n",
    "    \n",
    "    batch_norm1 = BatchNorm(hidden_size, activation='relu')\n",
    "    batch_norm2 = BatchNorm(hidden_size, activation='relu')\n",
    "    batch_norm3 = BatchNorm(hidden_size, activation='relu')\n",
    "    batch_norm4 = BatchNorm(hidden_size, activation='relu')\n",
    "    \n",
    "    model.add(dense1, 'dense1')\n",
    "    model.add(batch_norm1, 'batch_norm1')\n",
    "    model.add(dense2, 'dense2')\n",
    "    model.add(batch_norm2, 'batch_norm2')\n",
    "    model.add(dense3, 'dense3')\n",
    "    model.add(batch_norm3, 'batch_norm3')\n",
    "    model.add(dense4, 'dense4')\n",
    "    model.add(batch_norm4, 'batch_norm4')\n",
    "    model.add(dense5, 'dense5')\n",
    "    model.add_loss(cross_entropy_loss)\n",
    "    optimizer = Adam(1e-3, model)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = create_normalized_model(1e-2)\n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver(X_train_small, \n",
    "                                                                          y_train_small,\n",
    "                                                                          X_val,\n",
    "                                                                          y_val,\n",
    "                                                                          0.0,\n",
    "                                                                          optimizer,\n",
    "                                                                          epochs=10,\n",
    "                                                                          batch_size=50)\n",
    "\n",
    "\n",
    "loss_histories.append(loss_history)\n",
    "train_accuracy_histories.append(train_accuracy_history)\n",
    "val_accuracy_histories.append(val_accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "labels = [\"fully_connected\", \"normalized_fully_connected\"]\n",
    "for i in range(2):\n",
    "  plt.plot(train_accuracy_histories[i], '-', label=labels[i])\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for i in range(2):\n",
    "  plt.plot(val_accuracy_histories[i], '-', label=labels[i])\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "for i in range(2):\n",
    "  plt.plot(loss_histories[i], '-', label=labels[i])\n",
    "plt.title('Train loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<font size=\"5\">Réseau multi-couches et initialisation</font>\n",
    "\n",
    "Ceci est une recherche d'hyper-paramètre afin de trouver le meilleur *weight_scale*.  Ce paramètre sert lors de l'initialisation des poids du réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "weight_scales = np.logspace(-4, 0, num=20)\n",
    "\n",
    "best_train_accs, best_norm_train_accs = [], []\n",
    "best_val_accs, best_norm_val_accs = [], []\n",
    "mean_train_loss, mean_norm_train_loss = [], []\n",
    "\n",
    "for i, w in enumerate(weight_scales):\n",
    "    print(\"Running weight scale: %i / %i : %f\" % (i + 1, len(weight_scales), w))\n",
    "    optimizer = create_model(w)\n",
    "    norm_optimizer = create_normalized_model(w)\n",
    "    loss_history, train_accuracy_history, val_accuracy_history = epoch_solver(X_train_small, \n",
    "                                                                          y_train_small,\n",
    "                                                                          X_val,\n",
    "                                                                          y_val,\n",
    "                                                                          0.0,\n",
    "                                                                          optimizer,\n",
    "                                                                          epochs=10,\n",
    "                                                                          batch_size=50,\n",
    "                                                                          verbose=False)\n",
    "    best_train_accs.append(max(train_accuracy_history))\n",
    "    best_val_accs.append(max(val_accuracy_history))\n",
    "    mean_train_loss.append(np.mean(loss_history))\n",
    "    \n",
    "    loss_norm_history, train_norm_accuracy_history, val_norm_accuracy_history = epoch_solver(X_train_small, \n",
    "                                                                                             y_train_small,\n",
    "                                                                                             X_val,\n",
    "                                                                                             y_val,\n",
    "                                                                                             0.0,\n",
    "                                                                                             norm_optimizer,\n",
    "                                                                                             epochs=10,\n",
    "                                                                                             batch_size=50,\n",
    "                                                                                             verbose=False)\n",
    "    \n",
    "    best_norm_train_accs.append(max(train_norm_accuracy_history))\n",
    "    best_norm_val_accs.append(max(val_norm_accuracy_history))\n",
    "    mean_norm_train_loss.append(np.mean(loss_norm_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Best val accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best val accuracy')\n",
    "plt.semilogx(weight_scales, best_val_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, best_norm_val_accs, '-o', label='batchnorm')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Best train accuracy vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Best training accuracy')\n",
    "plt.semilogx(weight_scales, best_train_accs, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, best_norm_train_accs, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Final training loss vs weight initialization scale')\n",
    "plt.xlabel('Weight initialization scale')\n",
    "plt.ylabel('Final training loss')\n",
    "plt.semilogx(weight_scales, mean_train_loss, '-o', label='baseline')\n",
    "plt.semilogx(weight_scales, mean_norm_train_loss, '-o', label='batchnorm')\n",
    "plt.legend()\n",
    "\n",
    "plt.gcf().set_size_inches(10, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
