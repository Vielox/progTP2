{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp -r drive/MyDrive/IFT780/TP2/prog/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "Dans ce notebook, nous reproduirons le modèle conçu dans le notebook précédent, mais avec Pytorch !\n",
    "Pytorch est une bibliothèque permettant de concevoir et entraîner des réseaux de neurones facilement, gérant pour vous la propagation entre les couches, la rétropropagation, l'optimisation du modèle, l'utilisation du GPU, et plus.\n",
    "\n",
    "Pour commencer, il est conseillé de lire en bonne partie l'introduction à Pytorch ici:\n",
    "https://pytorch.org/tutorials/beginner/basics/intro.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10, display_images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd datasets\n",
    "!ls\n",
    "!bash -x ./get_datasets.sh\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.BatchNorm import SpatialBatchNorm\n",
    "from layers.Conv import Conv2DCython\n",
    "from layers.Dense import Dense\n",
    "from layers.Flatten import Flatten\n",
    "from layers.MaxPool import MaxPool2DCython\n",
    "from layers.Dropout import Dropout\n",
    "from model.Model import Model\n",
    "from utils.model_loss import cross_entropy_loss\n",
    "from model.Solver import epoch_solver, Adam\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500, num_batch=200):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cibles dev\n",
    "    - X_batch, y_batch: batch de données et de cibles \n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test, label_names = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    mask = range(num_batch)\n",
    "    X_batch = X_train[mask]\n",
    "    y_batch = y_train[mask]\n",
    "    \n",
    "    X_train = X_train.transpose(0, 3, 1, 2)\n",
    "    X_test = X_test.transpose(0, 3, 1, 2)\n",
    "    X_val = X_val.transpose(0, 3, 1, 2)\n",
    "    X_dev = X_dev.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev, X_batch, y_batch, label_names \n",
    "\n",
    "def preprocess_CIFAR10_data(X):\n",
    "\n",
    "    # Normalisation\n",
    "    X_mean = np.mean(X, axis = 0)\n",
    "    X_ = X - X_mean\n",
    "    \n",
    "    return X_\n",
    "\n",
    "Data_train, y_train, Data_val, y_val, Data_test, y_test, Data_dev, y_dev, Data_batch, y_batch, label_names = get_CIFAR10_data()\n",
    "X_train, X_val, X_test, X_dev, X_batch, = (preprocess_CIFAR10_data(Data) for Data in (Data_train, Data_val, Data_test, Data_dev, Data_batch))\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "print('batch data shape: ', X_batch.shape)\n",
    "print('batch labels shape: ', y_batch.shape)\n",
    "\n",
    "print(\"CIFAR10 est un jeu de données d'images RGB 32x32x3 séparées en 10 classes\")\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle en numpy\n",
    "\n",
    "Premièrement, recréons le modèle précédemment implémenté:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres de convolution, à modifier au besoin\n",
    "filter_size = 5\n",
    "channels = 3\n",
    "stride = 1\n",
    "p_dropout = 0.1\n",
    "pad = int((filter_size - 1)/2)\n",
    "\n",
    "# paramètres dense\n",
    "num_classes = 10\n",
    "\n",
    "def create_Nlayer_cnn(num_filter_layer1, num_filter_layer2, fc_size, init_weight_scale):\n",
    "\n",
    "    # TODO: Copier-coller ici votre architecture numpy de la dernière question\n",
    "    model = Model()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return model    \n",
    "\n",
    "reg = 1e-2 # à ajuster au besoin\n",
    "lr = 5e-4  # à ajuster au besoin\n",
    "model = create_Nlayer_cnn(32, 16, 400, 1e-2)\n",
    "\n",
    "optimizer = Adam(lr, model)\n",
    "    \n",
    "# On entraîne (1 epoch) avec 100% des données d'entraînement.\n",
    "loss_history, train_accuracy_history, val_accuracy_history = epoch_solver(X_train, \n",
    "                                                                          y_train,\n",
    "                                                                          X_val,\n",
    "                                                                          y_val,\n",
    "                                                                          reg,\n",
    "                                                                          optimizer,\n",
    "                                                                          batch_size=100,\n",
    "                                                                          epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history, '-o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Train : 50,000 images')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy_history, '-o')\n",
    "plt.plot(val_accuracy_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "test_acc = (y_test_pred == y_test).mean()\n",
    "print('Test accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle en pytorch\n",
    "\n",
    "Maintenant, recréons le modèle avec Pytorch. Tout d'abord, un peu de mise en place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se crée tout d'abord un dataset permettant d'acceuillir les données\n",
    "# À noter que Pytorch offre déjà des façons de charger CIFAR-10, mais pour les besoins de la cause\n",
    "# faisons-le nous-même:\n",
    "\n",
    "params = {'batch_size': 100,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 1}\n",
    "\n",
    "class CIFAR10Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, data, labels):\n",
    "        self.labels = labels\n",
    "        self.data = data\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Alors que le dataset permet d'accéder aux données, le \"DataLoader\" permet de les charger en \"batch\" (et\n",
    "# potentiellement avec plusieurs workers)\n",
    "train_pyt = CIFAR10Dataset(X_train, y_train)\n",
    "training_generator = torch.utils.data.DataLoader(train_pyt, **params)\n",
    "\n",
    "valid_pyt = CIFAR10Dataset(X_val, y_val)\n",
    "valid_generator = torch.utils.data.DataLoader(valid_pyt, **params)\n",
    "\n",
    "test_pyt = CIFAR10Dataset(X_test, y_test)\n",
    "test_generator = torch.utils.data.DataLoader(test_pyt, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres de convolution, à modifier au besoin\n",
    "filter_size = 5\n",
    "channels = 3\n",
    "stride = 1\n",
    "p_dropout = 0.5\n",
    "pad = int((filter_size - 1)/2)\n",
    "\n",
    "# paramètres dense\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# On se crée une classe pour notre modèle\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, stack: nn.Sequential):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.stack = stack\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.stack(x)\n",
    "        return out\n",
    "\n",
    "# On emboîte les layers dans nn.Sequential, permettant de simplifier le code de la propagation avant\n",
    "# Ce n'est pas nécéssaire en général, mais pour simplifier le code du travail pratique, on le fait comme ça:\n",
    "def create_cnn_stack(num_filter_layer1, num_filter_layer2, fc_size, init_weight_scale):\n",
    "    \n",
    "    # TODO: Convertir votre modèle numpy en modules pytorch   \n",
    "    # remplater -1 par le modèle créé\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 1e-2 # à ajuster au besoin\n",
    "lr = 5e-4  # à ajuster au besoin\n",
    "stack = create_cnn_stack(32, 16, 400, 1e-2)\n",
    "\n",
    "# On instancie le modèle en y passant la \"stack\" le couches\n",
    "model_pyt = NeuralNetwork(stack).to(device)\n",
    "\n",
    "# On se déclare un optimiseur qui effectuera la descente de gradient\n",
    "optimizer_pyt = optim.Adam(model_pyt.parameters(), lr=lr, weight_decay=reg)\n",
    "\n",
    "# On se déclare une loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_history, train_accuracy_history, valid_accuracy_history = [], [], []\n",
    "\n",
    "# On optimise ! Le code plus bas remplace la classe Solver précédemment définie\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    training_accuracy = []\n",
    "    valid_accuracy = []\n",
    "    for i, (batch, labels) in enumerate(training_generator):\n",
    "\n",
    "        batch = batch.float().to(device)\n",
    "        if device == 'cuda':\n",
    "            batch = batch.cuda()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = model_pyt(batch)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        accuracy = torch.sum(preds == labels) / len(batch)\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        optimizer_pyt.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_pyt.step()\n",
    "        \n",
    "        training_accuracy.append(accuracy)\n",
    "        loss_history.append(loss.detach().numpy())\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print('batch {} / {} loss : {}'.format(i, len(training_generator), loss))\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        for i, (batch, labels) in enumerate(valid_generator):\n",
    "            batch = batch.float().to(device)\n",
    "            if device == 'cuda':\n",
    "                batch = batch.cuda()\n",
    "            labels = labels.to(device)\n",
    "        \n",
    "            logits = model_pyt(batch)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            accuracy = torch.sum(preds == labels) / len(batch)\n",
    "            valid_accuracy.append(accuracy)\n",
    "            \n",
    "    mean_epoch_train_acc = torch.mean(torch.stack(training_accuracy))        \n",
    "    mean_epoch_valid_acc = torch.mean(torch.stack(valid_accuracy))     \n",
    "    train_accuracy_history.append(mean_epoch_train_acc)\n",
    "    valid_accuracy_history.append(mean_epoch_valid_acc)\n",
    "    print(f\"(epoch {epoch + 1} / {epochs}) loss: {loss}, train_acc: {mean_epoch_train_acc}, val_acc: {mean_epoch_valid_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history, '-o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Train : 50,000 images')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy_history, '-o')\n",
    "plt.plot(valid_accuracy_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tout s'est bien déroulé, vous devriez avoir une accuracy en validation et en test avec Pytorch semblable à celle en numpy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "test_accuracy = []\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for i, (batch, labels) in enumerate(test_generator):\n",
    "        batch = batch.float().to(device)\n",
    "        if device == 'cuda':\n",
    "            batch = batch.cuda()\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model_pyt(batch)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        accuracy = torch.sum(preds == labels) / len(batch)\n",
    "        if device == 'cuda':\n",
    "            accuracy = accuracy.cpu()\n",
    "            preds = preds.cpu()\n",
    "        test_accuracy.append(accuracy.numpy())\n",
    "        test_preds.append(preds.numpy())\n",
    "\n",
    "test_acc = np.mean(test_accuracy)\n",
    "y_test_pred = np.concatenate(test_preds, axis=0)\n",
    "print('Test accuracy: ', test_acc)\n",
    "\n",
    "display_images(np.swapaxes(np.swapaxes(Data_test,1,3),1,2), y_test_pred, label_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
