{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!cp -r drive/MyDrive/IFT780/TP2/prog/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# TP2: Dropout\n",
    "\n",
    "Dropout [1] est une technique de régularisation qui consiste à forcer aléatoirement à zéro certains neurones lors de la propagation avant. Pour cet exercice, vous serez appelé à coder une couche de dropout et de l'incorporer à votre réseau pleinement connecté.\n",
    "\n",
    "Ceci est le notebook le plus court du tp2.  Nous utiliserons pour l'essentiel le code dans les fichiers suivants :\n",
    "\n",
    "    model/Model.py\n",
    "    layers/Dropout.py \n",
    "    utils/model_loss.py\n",
    "    model/Solver.py\n",
    "    \n",
    "Comme au tp1, la classe **Model** \"crée\" un réseau de neurones en ajoutant successivement des couches et une fonction de perte.\n",
    "\n",
    "[1] Geoffrey E. Hinton et al, \"Improving neural networks by preventing co-adaptation of feature detectors\", arXiv 2012\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Pour automatiquement recharger les modules externes\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Charger la banque de données CIFAR-10, prétraiter les images et ajouter une dimension pour le biais.\n",
    "    \n",
    "    Input :\n",
    "    - num_training : nombre d'images à mettre dans l'ensemble d'entrainement\n",
    "    - num_validation : nombre d'images à mettre dans l'ensemble de validation\n",
    "    - num_test : nombre d'images à mettre dans l'ensemble de test\n",
    "    - num_dev : d'images à mettre dans l'ensemble dev\n",
    "    \n",
    "    Output :\n",
    "    - X_train, y_train : données et cibles d'entrainement\n",
    "    - X_val, y_val: données et cibles de validation\n",
    "    - X_test y_test: données et cibles de test \n",
    "    - X_dev, y_dev: données et cicles dev\n",
    "    \"\"\"\n",
    "    # Charger les données CIFAR-10\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test, label_names = load_CIFAR10(cifar10_dir)\n",
    "  \n",
    "    # Séparer en ensembles d'entraînement, de validation, de test et de dev\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalisation\n",
    "    X_train -= np.mean(X_train, axis = 0)\n",
    "    X_val -= np.mean(X_val, axis = 0)\n",
    "    X_test -= np.mean(X_test, axis = 0)\n",
    "    X_dev -= np.mean(X_dev, axis = 0)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "def preprocess_CIFAR10_data(X):\n",
    "\n",
    "    # Normalisation\n",
    "    X_mean = np.mean(X, axis = 0)\n",
    "    X_ = X - X_mean\n",
    "\n",
    "    # Ajout du biais\n",
    "    X_ = np.hstack([X_, np.ones((X.shape[0], 1))])\n",
    "    \n",
    "    return X_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3072)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 500)\n"
     ]
    }
   ],
   "source": [
    "from layers.Dense import Dense\n",
    "from layers.Dropout import Dropout\n",
    "from model.Model import Model\n",
    "from utils.model_loss import cross_entropy_loss\n",
    "\n",
    "def create_toy_data(shape):\n",
    "    np.random.seed(0)\n",
    "    return np.random.randn(*shape) + 10\n",
    "\n",
    "X = create_toy_data((500,500))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Propagation avant\n",
    "\n",
    "Dans le fichier `Dropout.py`, codez la propagation avant du dropout. Puisque dropout se comporte différemment en entraînement qu'en test, assurez-vous que les deux modes fonctionnent bien.\n",
    "\n",
    "Exécutez la cellule que voici et assurez-vous que la moyenne de out_train soit la même que out_test.\n",
    "\n",
    "NOTE : vous devez implémenter du \"inverse dropout\".  Pour plus de détail, voir https://deepnotes.io/dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests with p =  0.3\n",
      "Mean of input:  10.001537191397109\n",
      "Mean of train-time output:  9.99962697453269\n",
      "Mean of test-time output:  10.001537191397109\n",
      "Fraction of train-time output set to zero:  0.299984\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.6\n",
      "Mean of input:  10.001537191397109\n",
      "Mean of train-time output:  10.023981068432363\n",
      "Mean of test-time output:  10.001537191397109\n",
      "Fraction of train-time output set to zero:  0.59924\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n",
      "Running tests with p =  0.75\n",
      "Mean of input:  10.001537191397109\n",
      "Mean of train-time output:  10.067883930428835\n",
      "Mean of test-time output:  10.001537191397109\n",
      "Fraction of train-time output set to zero:  0.748408\n",
      "Fraction of test-time output set to zero:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode forward (propagation avant) de la classe de   #\n",
    "# couche Dropout. Le comportement lors de l'entraînement et des tests est    #\n",
    "# différent, assurez-vous donc que les deux fonctionnent.                    #\n",
    "##############################################################################\n",
    "\n",
    "for p in [0.3, 0.6, 0.75]:\n",
    "    dropout_layer = Dropout(drop_rate=p)\n",
    "    out_train = dropout_layer.forward(X, mode='train')\n",
    "    out_test = dropout_layer.forward(X, mode='test')\n",
    "\n",
    "    print('Running tests with p = ', p)\n",
    "    print('Mean of input: ', X.mean())\n",
    "    print('Mean of train-time output: ', out_train.mean())\n",
    "    print('Mean of test-time output: ', out_test.mean())\n",
    "    print('Fraction of train-time output set to zero: ', (out_train == 0).mean())\n",
    "    print('Fraction of test-time output set to zero: ', (out_test == 0).mean())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.gradients import evaluate_numerical_gradient\n",
    "\n",
    "# Retourne l'erreur relative maximale des matrices de gradients passées en paramètre.\n",
    "# Pour chaque paramètre, l'erreur relative devrait être inférieure à environ 1e-8.\n",
    "def rel_error(x, y):\n",
    "    rel = np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y)))\n",
    "    return np.max(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dropout: rétro-propagation\n",
    "Toujours dans le fichier `Dropout.py`, codez la rétro-propagation du dropout. Vous pourrez par la suite tester votre code avec la cellule que voici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################################################################\n",
    "# TODO: Implémenter la méthode backward (propagation arrière) de la classe de#\n",
    "# couche Dropout. Le comportement lors de l'entraînement et des tests est    #\n",
    "# différent, assurez-vous donc que les deux fonctionnent.                    #\n",
    "##############################################################################\n",
    "\n",
    "X = create_toy_data((10,10))\n",
    "dA = np.random.randn(*X.shape)\n",
    "\n",
    "dropout_layer = Dropout(drop_rate=0.8)\n",
    "\n",
    "_ = dropout_layer.forward(X, mode='train')\n",
    "dX = dropout_layer.backward(dA, mode='train')\n",
    "\n",
    "drop_mask = dropout_layer.cache\n",
    "\n",
    "# L'erreur relative devrait être très petite, inférieure à 1e-8\n",
    "rel_error(dX, drop_mask * dA  )  # Enlever le 0.2 si vous l'avez inclut dans dropout_layer.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Réseau multi-couches avec Dropout\n",
    "En principe, le code que voici devrait fonctionner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with dropout =  0\n",
      "Initial loss:  2.3008036853377414\n",
      "out =  [[-0.00145388 -0.00100286 -0.00111735 -0.00050843 -0.00138987  0.00031436\n",
      "  -0.00096323  0.00030955  0.00012466  0.00058227]\n",
      " [-0.00383029  0.0003986  -0.00233344 -0.00308759 -0.00210427 -0.00084433\n",
      "   0.00195573  0.00031301 -0.00210675  0.00378836]]\n",
      "dense1-W max relative error: 6.954661e-07\n",
      "dense1-b max relative error: 6.880290e-07\n",
      "dense2-W max relative error: 7.521862e-07\n",
      "dense2-b max relative error: 1.351341e-03\n",
      "dense3-W max relative error: 4.516915e-07\n",
      "dense3-b max relative error: 1.093557e-10\n",
      "Running check with dropout =  0.25\n",
      "Initial loss:  2.3011248122684687\n",
      "out =  [[ 2.36059594e-03  0.00000000e+00 -0.00000000e+00  7.40900877e-07\n",
      "  -1.81865689e-04  2.26279836e-03 -1.85803341e-03  4.27667690e-04\n",
      "   1.15828390e-03 -2.76774677e-04]\n",
      " [ 0.00000000e+00  5.11036225e-03 -5.41220475e-03  2.74234696e-03\n",
      "  -0.00000000e+00  3.24807668e-03  1.90017549e-03 -8.68556354e-04\n",
      "   1.76652322e-03  0.00000000e+00]]\n",
      "dense1-W max relative error: 1.000000e+00\n",
      "dense1-b max relative error: 1.000000e+00\n",
      "dense2-W max relative error: 1.000000e+00\n",
      "dense2-b max relative error: 1.000000e+00\n",
      "dense3-W max relative error: 1.000000e+00\n",
      "dense3-b max relative error: 1.000000e+00\n",
      "Running check with dropout =  0.5\n",
      "Initial loss:  2.304885110703462\n",
      "out =  [[-0.00110716  0.00162255  0.00059541  0.          0.         -0.003046\n",
      "  -0.00084974 -0.          0.          0.00260549]\n",
      " [ 0.00296478  0.00301985  0.00032632  0.          0.         -0.\n",
      "   0.          0.          0.00935777  0.        ]]\n",
      "dense1-W max relative error: 1.000000e+00\n",
      "dense1-b max relative error: 1.000000e+00\n",
      "dense2-W max relative error: 1.000000e+00\n",
      "dense2-b max relative error: 1.000000e+00\n",
      "dense3-W max relative error: 1.000000e+00\n",
      "dense3-b max relative error: 1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D) / 5\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "seed = 42\n",
    "\n",
    "for p in [0,0.25, 0.5]:\n",
    "    print('Running check with dropout = ', p)\n",
    "    model = Model()\n",
    "    \n",
    "    dense1 = Dense(dim_input=D, dim_output=H1, weight_scale=5e-2, activation='relu')\n",
    "    dropout1 = Dropout(drop_rate=p)\n",
    "    \n",
    "    dense2 = Dense(dim_input=H1, dim_output=H2, weight_scale=5e-2, activation='relu')\n",
    "    dropout2 = Dropout(drop_rate=p)\n",
    "    \n",
    "    dense3 = Dense(dim_input=H2, dim_output=C, weight_scale=5e-2)\n",
    "    dropout3 = Dropout(drop_rate=p)\n",
    "    \n",
    "    model.add(dense1, 'dense1')\n",
    "    model.add(dropout1, 'dropout1')\n",
    "    model.add(dense2, 'dense2')\n",
    "    model.add(dropout2, 'dropout2')\n",
    "    model.add(dense3, 'dense3')\n",
    "    model.add(dropout3, 'dropout3')\n",
    "    model.add_loss(cross_entropy_loss)\n",
    "    \n",
    "    \n",
    "    out = model.forward(X, seed=seed)\n",
    "    \n",
    "    loss, dScores, _ = model.calculate_loss(out, y, 0.0)\n",
    "    _ = model.backward(dScores)\n",
    "\n",
    "    gradients = model.gradients()\n",
    "    model_params = model.parameters()\n",
    "    print('Initial loss: ', loss)\n",
    "    print('out = ', out)\n",
    "    # Les erreurs devraient être inférieures ou égales à 1e-5\n",
    "    for layer_name, layer_params in model_params.items():\n",
    "        for param_name, _ in layer_params.items():\n",
    "            grad_num = evaluate_numerical_gradient(X, y, model, layer_name, param_name, reg=0.0, seed=seed)\n",
    "            max_error = rel_error(grad_num, gradients[layer_name][param_name])\n",
    "            # if p!=0 :\n",
    "            #     print(layer_name)\n",
    "            #     print('grad_num',grad_num)\n",
    "            #     print('gradient',gradients[layer_name][param_name])\n",
    "            print('%s max relative error: %e' % (layer_name + '-' + param_name, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Expérimentation\n",
    "Ici nous entrainerons 2 réseaux de neurones avec 500 données: l'un utilisera du dropout et l'autre non. Nous pourrons alors visualiser les justesses obtenues en entraînement et en validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'epoch_solver'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-983562333c35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m##############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSolver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mepoch_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'epoch_solver'"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Tester l'optimiseur Adam (voir fichier model/Solver.py)                    #\n",
    "##############################################################################\n",
    "\n",
    "from model.Solver import epoch_solver, Adam\n",
    "\n",
    "num_train = 500\n",
    "\n",
    "X_train_small = X_train[:num_train]\n",
    "y_train_small = y_train[:num_train]\n",
    "\n",
    "\n",
    "train_accuracy_histories = []\n",
    "val_accuracy_histories = []\n",
    "\n",
    "dropouts = [0, 0.3]\n",
    "\n",
    "for p in dropouts:\n",
    "    model = Model()\n",
    "    \n",
    "    dense1 = Dense(dim_output=500, weight_scale=1e-2, activation='relu')\n",
    "    dropout1 = Dropout(drop_rate=p)\n",
    "    \n",
    "    dense2 = Dense(dim_input=500, weight_scale=1e-2)\n",
    "    dropout2 = Dropout(drop_rate=p)\n",
    "    \n",
    "    model.add(dense1, 'dense1')\n",
    "    model.add(dropout1, 'dropout1')\n",
    "    model.add(dense2, 'dense2')\n",
    "    model.add(dropout2, 'dropout2')\n",
    "    model.add_loss(cross_entropy_loss)\n",
    "    \n",
    "    print('\\nDropout: ', p, '\\n')\n",
    "    \n",
    "    optimizer = Adam(1e-4, model)\n",
    "    \n",
    "    _, train_accuracy_history, val_accuracy_history = epoch_solver(X_train_small, \n",
    "                                                                   y_train_small,\n",
    "                                                                   X_val,\n",
    "                                                                   y_val,\n",
    "                                                                   0.0,\n",
    "                                                                   optimizer,\n",
    "                                                                   epochs=20)\n",
    "    \n",
    "    train_accuracy_histories.append(train_accuracy_history)\n",
    "    val_accuracy_histories.append(val_accuracy_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "for i in range(2):\n",
    "  plt.plot(train_accuracy_histories[i], '-', label='%.2f dropout' % dropouts[i])\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for i in range(2):\n",
    "  plt.plot(val_accuracy_histories[i], '-', label='%.2f dropout' % dropouts[i])\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01,\n",
       "       1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03,\n",
       "       2.98095799e+03])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issou =np.array([1,2,3,4,0,5,0,6,0,7,8,0,8])\n",
    "issou = np.exp(issou[issou!=0])\n",
    "issou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False,  True, False,  True, False,\n",
       "        True,  True, False,  True])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issou = np.array([1,2,3,4,0,5,0,6,0,7,8,0,8])\n",
    "issou != 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8, 8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issou[issou!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_mask = np.random.binomial(1, 0.25, 100)\n",
    "drop_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
